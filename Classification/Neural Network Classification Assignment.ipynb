{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4a871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#imports for neural network\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#https://keras.io/api/optimizers/ Different optimisers here. Can use different ones for report. Look into Adam parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123e3107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>diagnosis_Num</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>843786</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>844359</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84458202</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>844981</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>84501001</td>\n",
       "      <td>M</td>\n",
       "      <td>1</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  diagnosis_Num  radius_mean  texture_mean  \\\n",
       "0    842302         M              1        17.99         10.38   \n",
       "1    842517         M              1        20.57         17.77   \n",
       "2  84300903         M              1        19.69         21.25   \n",
       "3  84348301         M              1        11.42         20.38   \n",
       "4  84358402         M              1        20.29         14.34   \n",
       "5    843786         M              1        12.45         15.70   \n",
       "6    844359         M              1        18.25         19.98   \n",
       "7  84458202         M              1        13.71         20.83   \n",
       "8    844981         M              1        13.00         21.82   \n",
       "9  84501001         M              1        12.46         24.04   \n",
       "\n",
       "   perimeter_mean  area_mean  smoothness_mean  compactness_mean  \\\n",
       "0          122.80     1001.0          0.11840           0.27760   \n",
       "1          132.90     1326.0          0.08474           0.07864   \n",
       "2          130.00     1203.0          0.10960           0.15990   \n",
       "3           77.58      386.1          0.14250           0.28390   \n",
       "4          135.10     1297.0          0.10030           0.13280   \n",
       "5           82.57      477.1          0.12780           0.17000   \n",
       "6          119.60     1040.0          0.09463           0.10900   \n",
       "7           90.20      577.9          0.11890           0.16450   \n",
       "8           87.50      519.8          0.12730           0.19320   \n",
       "9           83.97      475.9          0.11860           0.23960   \n",
       "\n",
       "   concavity_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.30010  ...         25.38          17.33           184.60   \n",
       "1         0.08690  ...         24.99          23.41           158.80   \n",
       "2         0.19740  ...         23.57          25.53           152.50   \n",
       "3         0.24140  ...         14.91          26.50            98.87   \n",
       "4         0.19800  ...         22.54          16.67           152.20   \n",
       "5         0.15780  ...         15.47          23.75           103.40   \n",
       "6         0.11270  ...         22.88          27.66           153.20   \n",
       "7         0.09366  ...         17.06          28.14           110.60   \n",
       "8         0.18590  ...         15.49          30.73           106.20   \n",
       "9         0.22730  ...         15.09          40.68            97.65   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "5       741.6            0.1791             0.5249           0.5355   \n",
       "6      1606.0            0.1442             0.2576           0.3784   \n",
       "7       897.0            0.1654             0.3682           0.2678   \n",
       "8       739.3            0.1703             0.5401           0.5390   \n",
       "9       711.4            0.1853             1.0580           1.1050   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "5                0.1741          0.3985                  0.12440  \n",
       "6                0.1932          0.3063                  0.08368  \n",
       "7                0.1556          0.3196                  0.11510  \n",
       "8                0.2060          0.4378                  0.10720  \n",
       "9                0.2210          0.4366                  0.20750  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"breast_cancer.csv\")\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2068315a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "564    1\n",
       "565    1\n",
       "566    1\n",
       "567    1\n",
       "568    0\n",
       "Name: diagnosis_Num, Length: 569, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = dataset.iloc[ : ,3: ]\n",
    "Y = dataset.iloc[ : ,2]\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9654297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.07613</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>0.1794</td>\n",
       "      <td>0.05742</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>0.2196</td>\n",
       "      <td>0.07451</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07389</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.08243</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "5        12.45         15.70           82.57      477.1          0.12780   \n",
       "6        18.25         19.98          119.60     1040.0          0.09463   \n",
       "7        13.71         20.83           90.20      577.9          0.11890   \n",
       "8        13.00         21.82           87.50      519.8          0.12730   \n",
       "9        12.46         24.04           83.97      475.9          0.11860   \n",
       "\n",
       "   compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "0           0.27760         0.30010              0.14710         0.2419   \n",
       "1           0.07864         0.08690              0.07017         0.1812   \n",
       "2           0.15990         0.19740              0.12790         0.2069   \n",
       "3           0.28390         0.24140              0.10520         0.2597   \n",
       "4           0.13280         0.19800              0.10430         0.1809   \n",
       "5           0.17000         0.15780              0.08089         0.2087   \n",
       "6           0.10900         0.11270              0.07400         0.1794   \n",
       "7           0.16450         0.09366              0.05985         0.2196   \n",
       "8           0.19320         0.18590              0.09353         0.2350   \n",
       "9           0.23960         0.22730              0.08543         0.2030   \n",
       "\n",
       "   fractal_dimension_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "5                 0.07613  ...         15.47          23.75           103.40   \n",
       "6                 0.05742  ...         22.88          27.66           153.20   \n",
       "7                 0.07451  ...         17.06          28.14           110.60   \n",
       "8                 0.07389  ...         15.49          30.73           106.20   \n",
       "9                 0.08243  ...         15.09          40.68            97.65   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "5       741.6            0.1791             0.5249           0.5355   \n",
       "6      1606.0            0.1442             0.2576           0.3784   \n",
       "7       897.0            0.1654             0.3682           0.2678   \n",
       "8       739.3            0.1703             0.5401           0.5390   \n",
       "9       711.4            0.1853             1.0580           1.1050   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "5                0.1741          0.3985                  0.12440  \n",
       "6                0.1932          0.3063                  0.08368  \n",
       "7                0.1556          0.3196                  0.11510  \n",
       "8                0.2060          0.4378                  0.10720  \n",
       "9                0.2210          0.4366                  0.20750  \n",
       "\n",
       "[10 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6a8e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values in the DataFrame:\n",
      "id                         0\n",
      "diagnosis                  0\n",
      "diagnosis_Num              0\n",
      "radius_mean                0\n",
      "texture_mean               0\n",
      "perimeter_mean             0\n",
      "area_mean                  0\n",
      "smoothness_mean            0\n",
      "compactness_mean           0\n",
      "concavity_mean             0\n",
      "concave points_mean        0\n",
      "symmetry_mean              0\n",
      "fractal_dimension_mean     0\n",
      "radius_se                  0\n",
      "texture_se                 0\n",
      "perimeter_se               0\n",
      "area_se                    0\n",
      "smoothness_se              0\n",
      "compactness_se             0\n",
      "concavity_se               0\n",
      "concave points_se          0\n",
      "symmetry_se                0\n",
      "fractal_dimension_se       0\n",
      "radius_worst               0\n",
      "texture_worst              0\n",
      "perimeter_worst            0\n",
      "area_worst                 0\n",
      "smoothness_worst           0\n",
      "compactness_worst          0\n",
      "concavity_worst            0\n",
      "concave points_worst       0\n",
      "symmetry_worst             0\n",
      "fractal_dimension_worst    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "null_values = dataset.isnull().sum()\n",
    "\n",
    "print(\"Null values in the DataFrame:\")\n",
    "print(null_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a081d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some of the values may look between 0 and 1. We want to remove that numerical significance across the columns. \n",
    "#Remember a neuron fires between 0 and 1 so we need to squash all the valuues using min max formula.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mmScaler = MinMaxScaler()\n",
    "#astype async type\n",
    "X = mmScaler.fit_transform(X.astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59851afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d524a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the network\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation=\"relu\")) #(20, activation=\"relu\")) - baseline\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "model.add(Dense(30, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=tf.nn.softmax)) \n",
    "#Need two values for classification. Look into softmax.\n",
    "# The end layer neurons should always equal the amount of hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef9ddb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam', loss='sparse_categorical_crossentropy') #Adam optimizer - baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67d99ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "16/16 [==============================] - 1s 1ms/step - loss: 0.6775\n",
      "Epoch 2/300\n",
      "16/16 [==============================] - 0s 988us/step - loss: 0.6427\n",
      "Epoch 3/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.5829\n",
      "Epoch 4/300\n",
      "16/16 [==============================] - 0s 933us/step - loss: 0.4871\n",
      "Epoch 5/300\n",
      "16/16 [==============================] - 0s 925us/step - loss: 0.3657\n",
      "Epoch 6/300\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.2573\n",
      "Epoch 7/300\n",
      "16/16 [==============================] - 0s 891us/step - loss: 0.1967\n",
      "Epoch 8/300\n",
      "16/16 [==============================] - 0s 905us/step - loss: 0.1673\n",
      "Epoch 9/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1473\n",
      "Epoch 10/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1295\n",
      "Epoch 11/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1126\n",
      "Epoch 12/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.1013\n",
      "Epoch 13/300\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0886\n",
      "Epoch 14/300\n",
      "16/16 [==============================] - 0s 953us/step - loss: 0.0843\n",
      "Epoch 15/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0879\n",
      "Epoch 16/300\n",
      "16/16 [==============================] - 0s 995us/step - loss: 0.0800\n",
      "Epoch 17/300\n",
      "16/16 [==============================] - 0s 904us/step - loss: 0.0678\n",
      "Epoch 18/300\n",
      "16/16 [==============================] - 0s 972us/step - loss: 0.0652\n",
      "Epoch 19/300\n",
      "16/16 [==============================] - 0s 973us/step - loss: 0.0685\n",
      "Epoch 20/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0635\n",
      "Epoch 21/300\n",
      "16/16 [==============================] - 0s 870us/step - loss: 0.0585\n",
      "Epoch 22/300\n",
      "16/16 [==============================] - 0s 935us/step - loss: 0.0603\n",
      "Epoch 23/300\n",
      "16/16 [==============================] - 0s 941us/step - loss: 0.0569\n",
      "Epoch 24/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 25/300\n",
      "16/16 [==============================] - 0s 979us/step - loss: 0.0542\n",
      "Epoch 26/300\n",
      "16/16 [==============================] - 0s 983us/step - loss: 0.0667\n",
      "Epoch 27/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0660\n",
      "Epoch 28/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0529\n",
      "Epoch 29/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0537\n",
      "Epoch 30/300\n",
      "16/16 [==============================] - 0s 981us/step - loss: 0.0533\n",
      "Epoch 31/300\n",
      "16/16 [==============================] - 0s 941us/step - loss: 0.0554\n",
      "Epoch 32/300\n",
      "16/16 [==============================] - 0s 925us/step - loss: 0.0463\n",
      "Epoch 33/300\n",
      "16/16 [==============================] - 0s 974us/step - loss: 0.0453\n",
      "Epoch 34/300\n",
      "16/16 [==============================] - 0s 969us/step - loss: 0.0450\n",
      "Epoch 35/300\n",
      "16/16 [==============================] - 0s 990us/step - loss: 0.0501\n",
      "Epoch 36/300\n",
      "16/16 [==============================] - 0s 966us/step - loss: 0.0476\n",
      "Epoch 37/300\n",
      "16/16 [==============================] - 0s 979us/step - loss: 0.0429\n",
      "Epoch 38/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0455\n",
      "Epoch 39/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0409\n",
      "Epoch 40/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0417\n",
      "Epoch 41/300\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0396\n",
      "Epoch 42/300\n",
      "16/16 [==============================] - 0s 899us/step - loss: 0.0439\n",
      "Epoch 43/300\n",
      "16/16 [==============================] - 0s 948us/step - loss: 0.0481\n",
      "Epoch 44/300\n",
      "16/16 [==============================] - 0s 920us/step - loss: 0.0457\n",
      "Epoch 45/300\n",
      "16/16 [==============================] - 0s 969us/step - loss: 0.0403\n",
      "Epoch 46/300\n",
      "16/16 [==============================] - 0s 918us/step - loss: 0.0371\n",
      "Epoch 47/300\n",
      "16/16 [==============================] - 0s 954us/step - loss: 0.0412\n",
      "Epoch 48/300\n",
      "16/16 [==============================] - 0s 873us/step - loss: 0.0455\n",
      "Epoch 49/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0425\n",
      "Epoch 50/300\n",
      "16/16 [==============================] - 0s 970us/step - loss: 0.0484\n",
      "Epoch 51/300\n",
      "16/16 [==============================] - 0s 939us/step - loss: 0.0348\n",
      "Epoch 52/300\n",
      "16/16 [==============================] - 0s 976us/step - loss: 0.0348\n",
      "Epoch 53/300\n",
      "16/16 [==============================] - 0s 979us/step - loss: 0.0340\n",
      "Epoch 54/300\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.0360\n",
      "Epoch 55/300\n",
      "16/16 [==============================] - 0s 985us/step - loss: 0.0359\n",
      "Epoch 56/300\n",
      "16/16 [==============================] - 0s 944us/step - loss: 0.0480\n",
      "Epoch 57/300\n",
      "16/16 [==============================] - 0s 953us/step - loss: 0.0339\n",
      "Epoch 58/300\n",
      "16/16 [==============================] - 0s 868us/step - loss: 0.0305\n",
      "Epoch 59/300\n",
      "16/16 [==============================] - 0s 819us/step - loss: 0.0419\n",
      "Epoch 60/300\n",
      "16/16 [==============================] - 0s 960us/step - loss: 0.0436\n",
      "Epoch 61/300\n",
      "16/16 [==============================] - 0s 842us/step - loss: 0.0382\n",
      "Epoch 62/300\n",
      "16/16 [==============================] - 0s 907us/step - loss: 0.0320\n",
      "Epoch 63/300\n",
      "16/16 [==============================] - 0s 904us/step - loss: 0.0305\n",
      "Epoch 64/300\n",
      "16/16 [==============================] - 0s 821us/step - loss: 0.0312\n",
      "Epoch 65/300\n",
      "16/16 [==============================] - 0s 910us/step - loss: 0.0292\n",
      "Epoch 66/300\n",
      "16/16 [==============================] - 0s 872us/step - loss: 0.0436\n",
      "Epoch 67/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0401\n",
      "Epoch 68/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0367\n",
      "Epoch 69/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0407\n",
      "Epoch 70/300\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 0.0316\n",
      "Epoch 71/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0373\n",
      "Epoch 72/300\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.0313\n",
      "Epoch 73/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0355\n",
      "Epoch 74/300\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 0.0340\n",
      "Epoch 75/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0302\n",
      "Epoch 76/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0393\n",
      "Epoch 77/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 78/300\n",
      "16/16 [==============================] - 0s 897us/step - loss: 0.0714\n",
      "Epoch 79/300\n",
      "16/16 [==============================] - 0s 948us/step - loss: 0.0508\n",
      "Epoch 80/300\n",
      "16/16 [==============================] - 0s 898us/step - loss: 0.0365\n",
      "Epoch 81/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0305\n",
      "Epoch 82/300\n",
      "16/16 [==============================] - 0s 954us/step - loss: 0.0263\n",
      "Epoch 83/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0273\n",
      "Epoch 84/300\n",
      "16/16 [==============================] - 0s 986us/step - loss: 0.0297\n",
      "Epoch 85/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 86/300\n",
      "16/16 [==============================] - 0s 972us/step - loss: 0.0318\n",
      "Epoch 87/300\n",
      "16/16 [==============================] - 0s 902us/step - loss: 0.0248\n",
      "Epoch 88/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0307\n",
      "Epoch 89/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 90/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0246\n",
      "Epoch 91/300\n",
      "16/16 [==============================] - 0s 937us/step - loss: 0.0306\n",
      "Epoch 92/300\n",
      "16/16 [==============================] - 0s 995us/step - loss: 0.0237\n",
      "Epoch 93/300\n",
      "16/16 [==============================] - 0s 913us/step - loss: 0.0198\n",
      "Epoch 94/300\n",
      "16/16 [==============================] - 0s 966us/step - loss: 0.0450\n",
      "Epoch 95/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0588\n",
      "Epoch 96/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0529\n",
      "Epoch 97/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0477\n",
      "Epoch 98/300\n",
      "16/16 [==============================] - 0s 887us/step - loss: 0.0344\n",
      "Epoch 99/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0290\n",
      "Epoch 100/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0415\n",
      "Epoch 101/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0240\n",
      "Epoch 102/300\n",
      "16/16 [==============================] - 0s 920us/step - loss: 0.0253\n",
      "Epoch 103/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0283\n",
      "Epoch 104/300\n",
      "16/16 [==============================] - 0s 904us/step - loss: 0.0200\n",
      "Epoch 105/300\n",
      "16/16 [==============================] - 0s 977us/step - loss: 0.0223\n",
      "Epoch 106/300\n",
      "16/16 [==============================] - 0s 993us/step - loss: 0.0303\n",
      "Epoch 107/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0207\n",
      "Epoch 108/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0231\n",
      "Epoch 109/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0217\n",
      "Epoch 110/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 111/300\n",
      "16/16 [==============================] - 0s 958us/step - loss: 0.0220\n",
      "Epoch 112/300\n",
      "16/16 [==============================] - 0s 967us/step - loss: 0.0186\n",
      "Epoch 113/300\n",
      "16/16 [==============================] - 0s 977us/step - loss: 0.0187\n",
      "Epoch 114/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0182\n",
      "Epoch 115/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0186\n",
      "Epoch 116/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0226\n",
      "Epoch 117/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0277\n",
      "Epoch 118/300\n",
      "16/16 [==============================] - 0s 961us/step - loss: 0.0237\n",
      "Epoch 119/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0233\n",
      "Epoch 120/300\n",
      "16/16 [==============================] - 0s 878us/step - loss: 0.0218\n",
      "Epoch 121/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0159\n",
      "Epoch 122/300\n",
      "16/16 [==============================] - 0s 874us/step - loss: 0.0163\n",
      "Epoch 123/300\n",
      "16/16 [==============================] - 0s 924us/step - loss: 0.0216\n",
      "Epoch 124/300\n",
      "16/16 [==============================] - 0s 974us/step - loss: 0.0146\n",
      "Epoch 125/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0154\n",
      "Epoch 126/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0137\n",
      "Epoch 127/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0171\n",
      "Epoch 128/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0209\n",
      "Epoch 129/300\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.0168\n",
      "Epoch 130/300\n",
      "16/16 [==============================] - 0s 928us/step - loss: 0.0141\n",
      "Epoch 131/300\n",
      "16/16 [==============================] - 0s 921us/step - loss: 0.0150\n",
      "Epoch 132/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0147\n",
      "Epoch 133/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0145\n",
      "Epoch 134/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0125\n",
      "Epoch 135/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0138\n",
      "Epoch 136/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0153\n",
      "Epoch 137/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0200\n",
      "Epoch 138/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0190\n",
      "Epoch 139/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0133\n",
      "Epoch 140/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0113\n",
      "Epoch 141/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0132\n",
      "Epoch 142/300\n",
      "16/16 [==============================] - 0s 962us/step - loss: 0.0121\n",
      "Epoch 143/300\n",
      "16/16 [==============================] - 0s 977us/step - loss: 0.0139\n",
      "Epoch 144/300\n",
      "16/16 [==============================] - 0s 935us/step - loss: 0.0115\n",
      "Epoch 145/300\n",
      "16/16 [==============================] - 0s 900us/step - loss: 0.0104\n",
      "Epoch 146/300\n",
      "16/16 [==============================] - 0s 963us/step - loss: 0.0126\n",
      "Epoch 147/300\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.0130\n",
      "Epoch 148/300\n",
      "16/16 [==============================] - 0s 942us/step - loss: 0.0174\n",
      "Epoch 149/300\n",
      "16/16 [==============================] - 0s 956us/step - loss: 0.0476\n",
      "Epoch 150/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0418\n",
      "Epoch 151/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0109\n",
      "Epoch 152/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0116\n",
      "Epoch 153/300\n",
      "16/16 [==============================] - 0s 965us/step - loss: 0.0097\n",
      "Epoch 154/300\n",
      "16/16 [==============================] - 0s 984us/step - loss: 0.0210\n",
      "Epoch 155/300\n",
      "16/16 [==============================] - 0s 949us/step - loss: 0.0184\n",
      "Epoch 156/300\n",
      "16/16 [==============================] - 0s 992us/step - loss: 0.0120\n",
      "Epoch 157/300\n",
      "16/16 [==============================] - 0s 980us/step - loss: 0.0096\n",
      "Epoch 158/300\n",
      "16/16 [==============================] - 0s 995us/step - loss: 0.0079\n",
      "Epoch 159/300\n",
      "16/16 [==============================] - 0s 977us/step - loss: 0.0123\n",
      "Epoch 160/300\n",
      "16/16 [==============================] - 0s 962us/step - loss: 0.0080\n",
      "Epoch 161/300\n",
      "16/16 [==============================] - 0s 998us/step - loss: 0.0081\n",
      "Epoch 162/300\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.0089\n",
      "Epoch 163/300\n",
      "16/16 [==============================] - 0s 950us/step - loss: 0.0079\n",
      "Epoch 164/300\n",
      "16/16 [==============================] - 0s 963us/step - loss: 0.0135\n",
      "Epoch 165/300\n",
      "16/16 [==============================] - 0s 939us/step - loss: 0.0093\n",
      "Epoch 166/300\n",
      "16/16 [==============================] - 0s 957us/step - loss: 0.0189\n",
      "Epoch 167/300\n",
      "16/16 [==============================] - 0s 944us/step - loss: 0.0098\n",
      "Epoch 168/300\n",
      "16/16 [==============================] - 0s 884us/step - loss: 0.0096\n",
      "Epoch 169/300\n",
      "16/16 [==============================] - 0s 988us/step - loss: 0.0111\n",
      "Epoch 170/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0192\n",
      "Epoch 171/300\n",
      "16/16 [==============================] - 0s 995us/step - loss: 0.1060\n",
      "Epoch 172/300\n",
      "16/16 [==============================] - 0s 939us/step - loss: 0.0986\n",
      "Epoch 173/300\n",
      "16/16 [==============================] - 0s 972us/step - loss: 0.0410\n",
      "Epoch 174/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0236\n",
      "Epoch 175/300\n",
      "16/16 [==============================] - 0s 969us/step - loss: 0.0162\n",
      "Epoch 176/300\n",
      "16/16 [==============================] - 0s 948us/step - loss: 0.0181\n",
      "Epoch 177/300\n",
      "16/16 [==============================] - 0s 973us/step - loss: 0.0212\n",
      "Epoch 178/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0131\n",
      "Epoch 179/300\n",
      "16/16 [==============================] - 0s 974us/step - loss: 0.0088\n",
      "Epoch 180/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0072\n",
      "Epoch 181/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0069\n",
      "Epoch 182/300\n",
      "16/16 [==============================] - 0s 914us/step - loss: 0.0063\n",
      "Epoch 183/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 184/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0064\n",
      "Epoch 185/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0067\n",
      "Epoch 186/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0055\n",
      "Epoch 187/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0049\n",
      "Epoch 188/300\n",
      "16/16 [==============================] - 0s 908us/step - loss: 0.0067\n",
      "Epoch 189/300\n",
      "16/16 [==============================] - 0s 913us/step - loss: 0.0068\n",
      "Epoch 190/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0085\n",
      "Epoch 191/300\n",
      "16/16 [==============================] - 0s 938us/step - loss: 0.0080\n",
      "Epoch 192/300\n",
      "16/16 [==============================] - 0s 999us/step - loss: 0.0050\n",
      "Epoch 193/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0081\n",
      "Epoch 194/300\n",
      "16/16 [==============================] - 0s 942us/step - loss: 0.0039\n",
      "Epoch 195/300\n",
      "16/16 [==============================] - 0s 956us/step - loss: 0.0042\n",
      "Epoch 196/300\n",
      "16/16 [==============================] - 0s 932us/step - loss: 0.0041\n",
      "Epoch 197/300\n",
      "16/16 [==============================] - 0s 923us/step - loss: 0.0042\n",
      "Epoch 198/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 978us/step - loss: 0.0037\n",
      "Epoch 199/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0034\n",
      "Epoch 200/300\n",
      "16/16 [==============================] - 0s 948us/step - loss: 0.0035\n",
      "Epoch 201/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0034\n",
      "Epoch 202/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0046\n",
      "Epoch 203/300\n",
      "16/16 [==============================] - 0s 941us/step - loss: 0.0074\n",
      "Epoch 204/300\n",
      "16/16 [==============================] - 0s 950us/step - loss: 0.0118\n",
      "Epoch 205/300\n",
      "16/16 [==============================] - 0s 871us/step - loss: 0.0068\n",
      "Epoch 206/300\n",
      "16/16 [==============================] - 0s 906us/step - loss: 0.0036\n",
      "Epoch 207/300\n",
      "16/16 [==============================] - 0s 873us/step - loss: 0.0035\n",
      "Epoch 208/300\n",
      "16/16 [==============================] - 0s 960us/step - loss: 0.0039\n",
      "Epoch 209/300\n",
      "16/16 [==============================] - 0s 908us/step - loss: 0.0041\n",
      "Epoch 210/300\n",
      "16/16 [==============================] - 0s 909us/step - loss: 0.0028\n",
      "Epoch 211/300\n",
      "16/16 [==============================] - 0s 944us/step - loss: 0.0027\n",
      "Epoch 212/300\n",
      "16/16 [==============================] - 0s 873us/step - loss: 0.0031\n",
      "Epoch 213/300\n",
      "16/16 [==============================] - 0s 926us/step - loss: 0.0029\n",
      "Epoch 214/300\n",
      "16/16 [==============================] - 0s 928us/step - loss: 0.0030\n",
      "Epoch 215/300\n",
      "16/16 [==============================] - 0s 928us/step - loss: 0.0024\n",
      "Epoch 216/300\n",
      "16/16 [==============================] - 0s 897us/step - loss: 0.0022\n",
      "Epoch 217/300\n",
      "16/16 [==============================] - 0s 960us/step - loss: 0.0029\n",
      "Epoch 218/300\n",
      "16/16 [==============================] - 0s 869us/step - loss: 0.0048\n",
      "Epoch 219/300\n",
      "16/16 [==============================] - 0s 968us/step - loss: 0.0065\n",
      "Epoch 220/300\n",
      "16/16 [==============================] - 0s 922us/step - loss: 0.0028\n",
      "Epoch 221/300\n",
      "16/16 [==============================] - 0s 862us/step - loss: 0.0024\n",
      "Epoch 222/300\n",
      "16/16 [==============================] - 0s 936us/step - loss: 0.0019\n",
      "Epoch 223/300\n",
      "16/16 [==============================] - 0s 863us/step - loss: 0.0017\n",
      "Epoch 224/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0016\n",
      "Epoch 225/300\n",
      "16/16 [==============================] - 0s 955us/step - loss: 0.0020\n",
      "Epoch 226/300\n",
      "16/16 [==============================] - 0s 915us/step - loss: 0.0036\n",
      "Epoch 227/300\n",
      "16/16 [==============================] - 0s 932us/step - loss: 0.0034\n",
      "Epoch 228/300\n",
      "16/16 [==============================] - 0s 918us/step - loss: 0.0023\n",
      "Epoch 229/300\n",
      "16/16 [==============================] - 0s 934us/step - loss: 0.0015\n",
      "Epoch 230/300\n",
      "16/16 [==============================] - 0s 928us/step - loss: 0.0013\n",
      "Epoch 231/300\n",
      "16/16 [==============================] - 0s 891us/step - loss: 0.0019\n",
      "Epoch 232/300\n",
      "16/16 [==============================] - 0s 909us/step - loss: 0.0015\n",
      "Epoch 233/300\n",
      "16/16 [==============================] - 0s 843us/step - loss: 0.0015\n",
      "Epoch 234/300\n",
      "16/16 [==============================] - 0s 869us/step - loss: 0.0017\n",
      "Epoch 235/300\n",
      "16/16 [==============================] - 0s 865us/step - loss: 0.0010\n",
      "Epoch 236/300\n",
      "16/16 [==============================] - 0s 935us/step - loss: 0.0024\n",
      "Epoch 237/300\n",
      "16/16 [==============================] - 0s 910us/step - loss: 0.0178\n",
      "Epoch 238/300\n",
      "16/16 [==============================] - 0s 859us/step - loss: 0.0216\n",
      "Epoch 239/300\n",
      "16/16 [==============================] - 0s 940us/step - loss: 0.0115\n",
      "Epoch 240/300\n",
      "16/16 [==============================] - 0s 943us/step - loss: 0.0514\n",
      "Epoch 241/300\n",
      "16/16 [==============================] - 0s 947us/step - loss: 0.0361\n",
      "Epoch 242/300\n",
      "16/16 [==============================] - 0s 944us/step - loss: 0.0225\n",
      "Epoch 243/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0324\n",
      "Epoch 244/300\n",
      "16/16 [==============================] - 0s 986us/step - loss: 0.0351\n",
      "Epoch 245/300\n",
      "16/16 [==============================] - 0s 934us/step - loss: 0.0136\n",
      "Epoch 246/300\n",
      "16/16 [==============================] - 0s 926us/step - loss: 0.0103\n",
      "Epoch 247/300\n",
      "16/16 [==============================] - 0s 920us/step - loss: 0.0045\n",
      "Epoch 248/300\n",
      "16/16 [==============================] - 0s 924us/step - loss: 0.0065\n",
      "Epoch 249/300\n",
      "16/16 [==============================] - 0s 907us/step - loss: 0.0036\n",
      "Epoch 250/300\n",
      "16/16 [==============================] - 0s 861us/step - loss: 0.0049\n",
      "Epoch 251/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0025\n",
      "Epoch 252/300\n",
      "16/16 [==============================] - 0s 991us/step - loss: 0.0072\n",
      "Epoch 253/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0047\n",
      "Epoch 254/300\n",
      "16/16 [==============================] - 0s 973us/step - loss: 0.0015\n",
      "Epoch 255/300\n",
      "16/16 [==============================] - 0s 945us/step - loss: 0.0017\n",
      "Epoch 256/300\n",
      "16/16 [==============================] - 0s 932us/step - loss: 0.0025\n",
      "Epoch 257/300\n",
      "16/16 [==============================] - 0s 888us/step - loss: 0.0014\n",
      "Epoch 258/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0014\n",
      "Epoch 259/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 260/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 261/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0013\n",
      "Epoch 262/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0014\n",
      "Epoch 263/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0014\n",
      "Epoch 264/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 265/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0013\n",
      "Epoch 266/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.0010\n",
      "Epoch 267/300\n",
      "16/16 [==============================] - 0s 950us/step - loss: 0.0012\n",
      "Epoch 268/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.6382e-04\n",
      "Epoch 269/300\n",
      "16/16 [==============================] - 0s 965us/step - loss: 9.9515e-04\n",
      "Epoch 270/300\n",
      "16/16 [==============================] - 0s 943us/step - loss: 9.5377e-04\n",
      "Epoch 271/300\n",
      "16/16 [==============================] - 0s 925us/step - loss: 9.9683e-04\n",
      "Epoch 272/300\n",
      "16/16 [==============================] - 0s 978us/step - loss: 9.3749e-04\n",
      "Epoch 273/300\n",
      "16/16 [==============================] - 0s 852us/step - loss: 9.4435e-04\n",
      "Epoch 274/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.7146e-04\n",
      "Epoch 275/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.7871e-04\n",
      "Epoch 276/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.4736e-04\n",
      "Epoch 277/300\n",
      "16/16 [==============================] - 0s 997us/step - loss: 8.8756e-04\n",
      "Epoch 278/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.8396e-04\n",
      "Epoch 279/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 9.5733e-04\n",
      "Epoch 280/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 8.5707e-04\n",
      "Epoch 281/300\n",
      "16/16 [==============================] - 0s 972us/step - loss: 9.2146e-04\n",
      "Epoch 282/300\n",
      "16/16 [==============================] - 0s 993us/step - loss: 8.0882e-04\n",
      "Epoch 283/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.4480e-04\n",
      "Epoch 284/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 7.2326e-04\n",
      "Epoch 285/300\n",
      "16/16 [==============================] - 0s 808us/step - loss: 7.0565e-04\n",
      "Epoch 286/300\n",
      "16/16 [==============================] - 0s 845us/step - loss: 6.9207e-04\n",
      "Epoch 287/300\n",
      "16/16 [==============================] - 0s 907us/step - loss: 6.5684e-04\n",
      "Epoch 288/300\n",
      "16/16 [==============================] - 0s 871us/step - loss: 6.7603e-04\n",
      "Epoch 289/300\n",
      "16/16 [==============================] - 0s 971us/step - loss: 6.2266e-04\n",
      "Epoch 290/300\n",
      "16/16 [==============================] - 0s 920us/step - loss: 6.0203e-04\n",
      "Epoch 291/300\n",
      "16/16 [==============================] - 0s 917us/step - loss: 7.0063e-04\n",
      "Epoch 292/300\n",
      "16/16 [==============================] - 0s 864us/step - loss: 6.9683e-04\n",
      "Epoch 293/300\n",
      "16/16 [==============================] - 0s 937us/step - loss: 7.0625e-04\n",
      "Epoch 294/300\n",
      "16/16 [==============================] - 0s 937us/step - loss: 5.5135e-04\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 870us/step - loss: 5.7850e-04\n",
      "Epoch 296/300\n",
      "16/16 [==============================] - 0s 864us/step - loss: 5.4787e-04\n",
      "Epoch 297/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.4910e-04\n",
      "Epoch 298/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.7377e-04\n",
      "Epoch 299/300\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 5.2505e-04\n",
      "Epoch 300/300\n",
      "16/16 [==============================] - 0s 990us/step - loss: 5.1939e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ec48abdf90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train.values, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05312e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "#predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "187d05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(0, len(X_test)):\n",
    "    #print(\"Predicted = \", np.argmax(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9609d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ead098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Processing so we can compare the predicted results:\n",
    "predictionList = []\n",
    "\n",
    "for i in range(0, len(X_test)):\n",
    "    predictionList.append(np.argmax(predictions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "797a936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "predictionsFrame = DataFrame(predictionList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5c0197f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99        35\n",
      "           1       0.96      1.00      0.98        22\n",
      "\n",
      "    accuracy                           0.98        57\n",
      "   macro avg       0.98      0.99      0.98        57\n",
      "weighted avg       0.98      0.98      0.98        57\n",
      "\n",
      "Confusion Matrix: \n",
      "[[34  1]\n",
      " [ 0 22]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictionsFrame))\n",
    "print(\"Confusion Matrix: \")\n",
    "print(confusion_matrix(y_test, predictionsFrame))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66b171ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#At predicting 0 it got 35 correct and 0 wrong.\n",
    "#At predicting 1 it got 21 correct and 1 wrong.\n",
    "\n",
    "# CHANGE TEST_SIZE AND RANDOM_STATE for report.\n",
    "# Also settings with adam optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61de02bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building confusion matrix plot\n",
    "\n",
    "cm = confusion_matrix(y_test, predictionsFrame)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap=\"Greens\", fmt=\"d\", \n",
    "            xticklabels=[\"0\", \"1\"], yticklabels=[\"0\", \"1\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7cf3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
